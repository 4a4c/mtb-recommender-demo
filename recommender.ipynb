{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f096ee0",
   "metadata": {},
   "source": [
    "# 1. Setup / imports (initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data libraries\n",
    "import pandas as pd            # Data manipulation (DataFrame) and CSV I/O\n",
    "import numpy as np             # Numerical arrays, random sampling and math utilities\n",
    "\n",
    "# Surprise (scikit-surprise) — recommender-system algorithms and helpers\n",
    "# - Dataset/Reader help load pandas DataFrames into Surprise format\n",
    "# - SVD is the matrix-factorization algorithm used for recommendations\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy     # RMSE and other evaluation helpers\n",
    "\n",
    "# scikit-learn utilities\n",
    "from sklearn.metrics import ndcg_score  # ranking metric used to evaluate top-k recommendations\n",
    "\n",
    "# Suppress verbose or harmless warnings in notebook output to keep logs clean\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Quick sanity print showing imports succeeded when this cell runs\n",
    "print(\"All libraries imported – ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111075ea",
   "metadata": {},
   "source": [
    "# 2. Data generation (brands + synthetic ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7de53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mountain bike brands and their market share weights\n",
    "# Weights are based on hypothetical 2024 Pinkbike Community Survey results\n",
    "# Higher weights (e.g., 0.20) indicate more popular brands that tend to get better ratings\n",
    "brands = ['Specialized', 'Santa Cruz', 'Transition', 'Trek', 'Giant',\n",
    "          'Pivot', 'Rocky Mountain', 'Forbidden', 'Commencal', 'Canyon']\n",
    "popularity_weights = [0.20, 0.18, 0.16, 0.15, 0.05, 0.08, 0.07, 0.04, 0.04, 0.03]\n",
    "\n",
    "# Generate synthetic rating data:\n",
    "# - Each user rates 3-5 brands randomly (simulating real-world partial ratings)\n",
    "# - More popular brands (higher weights) tend to get better ratings\n",
    "# - Add some random noise to make it realistic\n",
    "np.random.seed(42)                     # Set seed for reproducible results\n",
    "num_users = 500                        # Number of synthetic users to generate\n",
    "data = []\n",
    "\n",
    "for user_id in range(1, num_users + 1):\n",
    "    # Each user rates a random number (3-5) of brands\n",
    "    n = np.random.randint(3, 6)\n",
    "    rated = np.random.choice(brands, n, replace=False)  # No duplicate ratings per user\n",
    "    \n",
    "    for brand in rated:\n",
    "        # Calculate rating based on brand popularity (weight) plus random noise:\n",
    "        # - Base rating: 3 (neutral)\n",
    "        # - Brand effect: 2 * weight (0.06 to 0.40 boost for brand popularity)\n",
    "        # - Random noise: Normal dist. with σ=0.5\n",
    "        # - Final rating clipped to valid range [1,5] and rounded\n",
    "        w = popularity_weights[brands.index(brand)]\n",
    "        rating = np.clip(np.random.normal(3 + 2*w, 0.5), 1, 5)\n",
    "        data.append({'user_id': user_id, \n",
    "                    'brand': brand,\n",
    "                    'rating': round(rating)})  # Round to whole numbers\n",
    "\n",
    "# Convert to DataFrame and show summary\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Generated {len(df)} ratings from {num_users} users\")\n",
    "# Display all ratings for example users 1, 10, 50 and 100\n",
    "selected_users = user_ids if 'user_ids' in globals() else [1, 10, 50, 100]\n",
    "df[df['user_id'].isin(selected_users)].sort_values(['user_id', 'brand']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1166f42",
   "metadata": {},
   "source": [
    "# 3. Export / summary (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ad994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the synthetic ratings dataset to CSV\n",
    "# This makes it easy to analyze the data outside the notebook\n",
    "df.to_csv('mtb_ratings.csv', index=False)\n",
    "print(\"Dataset exported to mtb_ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876a0ac",
   "metadata": {},
   "source": [
    "# 4. Train & recommend (modeling cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic DataFrame into Surprise's Dataset format\n",
    "# Reader tells Surprise the rating scale used in the DataFrame\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# Dataset.load_from_df expects columns in the order: user, item, rating\n",
    "data = Dataset.load_from_df(df[['user_id', 'brand', 'rating']], reader)\n",
    "\n",
    "# Split into training and test sets; test_size=0.2 uses 20% of ratings as hold-out\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train a matrix-factorization model (SVD) on the training set\n",
    "# SVD here is from surprise and implements a baseline matrix factorization algorithm\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# ---- Demo recommendations for a few sample users ----\n",
    "# We choose a representative set of user ids across the range to demonstrate predictions\n",
    "user_ids = [1, 10, 50, 100]  # Example user ids to inspect recommendations\n",
    "all_brands = brands  # use the existing brands list generated earlier\n",
    "\n",
    "# For each user, compute predictions for items the user has NOT rated and show top-3\n",
    "for user_id in user_ids:\n",
    "    # Extract brands this user already rated so we avoid recommending them\n",
    "    already_rated = df[df['user_id'] == user_id]['brand'].tolist()\n",
    "\n",
    "    # Predict rating for each brand the user hasn't rated yet\n",
    "    preds = [model.predict(user_id, b) for b in all_brands if b not in already_rated]\n",
    "\n",
    "    # Sort predictions by estimated rating and pick top 3\n",
    "    top3 = sorted(preds, key=lambda x: x.est, reverse=True)[:3]\n",
    "\n",
    "    # Print nicely\n",
    "    print(f\"\\nTop-3 recommendations for User {user_id}:\")\n",
    "    for i, p in enumerate(top3, 1):\n",
    "        # p.iid is the item id (brand), p.est is the predicted rating\n",
    "        print(f\"  {i}. {p.iid}  →  {p.est:.2f}\")\n",
    "\n",
    "# Note: the block below was previously duplicated in the cell. The loop above\n",
    "# already prints recommendations for the selected users, so we won't repeat it.\n",
    "\n",
    "# ---- Evaluation on the hold-out test set ----\n",
    "# Use the trained model to predict ratings for the test set (the hold-out 20%)\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# RMSE is a common accuracy metric for rating prediction\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "\n",
    "# Compute nDCG per user to measure ranking quality\n",
    "# We'll skip users with only a single test item because nDCG requires at least 2 items\n",
    "ndcgs = []\n",
    "for uid in {p.uid for p in predictions}:\n",
    "    # Collect all predictions for this user\n",
    "    ups = [p for p in predictions if p.uid == uid]\n",
    "    if len(ups) > 1:\n",
    "        # true relevance scores and estimated scores\n",
    "        true = [p.r_ui for p in ups]\n",
    "        est  = [p.est  for p in ups]\n",
    "        # ndcg_score expects a list-of-lists for multiple queries, so wrap in []\n",
    "        ndcgs.append(ndcg_score([true], [est]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86f6e7",
   "metadata": {},
   "source": [
    "# 5. Evaluation (evaluation cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Comprehensive Model Evaluation ----\n",
    "# Test the model on the 20% hold-out set to evaluate performance\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# 1. Rating Prediction Accuracy (RMSE)\n",
    "# - Measures how close our predicted ratings are to actual ratings\n",
    "# - Lower is better (range: 0 to 4 for 5-star scale)\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "\n",
    "# 2. Ranking Quality (nDCG)\n",
    "# - Measures how well we order recommendations\n",
    "# - Accounts for both relevance and position in ranked list\n",
    "# - Range: 0 to 1 (1 = perfect ranking)\n",
    "ndcgs = []\n",
    "ndcg_at_k = []  # Also compute nDCG@3 for top-K evaluation\n",
    "\n",
    "# Process predictions per user\n",
    "for uid in {p.uid for p in predictions}:\n",
    "    # Get all predictions for this user\n",
    "    ups = [p for p in predictions if p.uid == uid]\n",
    "    if len(ups) > 1:  # Need at least 2 items for meaningful ranking\n",
    "        # Sort by actual and predicted ratings\n",
    "        true = [p.r_ui for p in ups]\n",
    "        est = [p.est for p in ups]\n",
    "        \n",
    "        # Calculate full nDCG\n",
    "        ndcgs.append(ndcg_score([true], [est]))\n",
    "        \n",
    "        # Calculate nDCG@3 (for top-3 recommendations)\n",
    "        if len(ups) >= 3:\n",
    "            ndcg_at_k.append(ndcg_score([true[:3]], [est[:3]]))\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_ndcg = np.mean(ndcgs) if ndcgs else 0.0\n",
    "mean_ndcg_at_3 = np.mean(ndcg_at_k) if ndcg_at_k else 0.0\n",
    "\n",
    "# 3. Print Comprehensive Evaluation Results\n",
    "print(\"\\nModel Evaluation Results (20% hold-out):\")\n",
    "print(\"\\nRating Prediction Accuracy:\")\n",
    "print(f\"  RMSE        : {rmse:.3f}    # How accurate are the predicted ratings?\")\n",
    "print(f\"                              # Target: < 1.2 for 5-star systems\")\n",
    "\n",
    "print(\"\\nRanking Quality:\")\n",
    "print(f\"  Mean nDCG   : {mean_ndcg:.3f}    # How well do we rank all items?\")\n",
    "print(f\"  nDCG@3      : {mean_ndcg_at_3:.3f}    # How good are our top-3 recommendations?\")\n",
    "print(f\"                              # Target: > 0.5 for useful recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
