{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data libraries\n",
    "import pandas as pd            # Data manipulation (DataFrame) and CSV I/O\n",
    "import numpy as np             # Numerical arrays, random sampling and math utilities\n",
    "\n",
    "# Surprise (scikit-surprise) — recommender-system algorithms and helpers\n",
    "# - Dataset/Reader help load pandas DataFrames into Surprise format\n",
    "# - SVD is the matrix-factorization algorithm used for recommendations\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy     # RMSE and other evaluation helpers\n",
    "\n",
    "# scikit-learn utilities\n",
    "from sklearn.metrics import ndcg_score  # ranking metric used to evaluate top-k recommendations\n",
    "\n",
    "# Suppress verbose or harmless warnings in notebook output to keep logs clean\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Quick sanity print showing imports succeeded when this cell runs\n",
    "print(\"All libraries imported – ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7de53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mountain bike brands and their market share weights\n",
    "# Weights are based on hypothetical 2024 Pinkbike Community Survey results\n",
    "# Higher weights (e.g., 0.20) indicate more popular brands that tend to get better ratings\n",
    "brands = ['Specialized', 'Santa Cruz', 'Transition', 'Trek', 'Giant',\n",
    "          'Pivot', 'Rocky Mountain', 'Forbidden', 'Commencal', 'Canyon']\n",
    "popularity_weights = [0.20, 0.18, 0.16, 0.15, 0.05, 0.08, 0.07, 0.04, 0.04, 0.03]\n",
    "\n",
    "# Generate synthetic rating data:\n",
    "# - Each user rates 3-5 brands randomly (simulating real-world partial ratings)\n",
    "# - More popular brands (higher weights) tend to get better ratings\n",
    "# - Add some random noise to make it realistic\n",
    "np.random.seed(42)                     # Set seed for reproducible results\n",
    "num_users = 500                        # Number of synthetic users to generate\n",
    "data = []\n",
    "\n",
    "for user_id in range(1, num_users + 1):\n",
    "    # Each user rates a random number (3-5) of brands\n",
    "    n = np.random.randint(3, 6)\n",
    "    rated = np.random.choice(brands, n, replace=False)  # No duplicate ratings per user\n",
    "    \n",
    "    for brand in rated:\n",
    "        # Calculate rating based on brand popularity (weight) plus random noise:\n",
    "        # - Base rating: 3 (neutral)\n",
    "        # - Brand effect: 2 * weight (0.06 to 0.40 boost for brand popularity)\n",
    "        # - Random noise: Normal dist. with σ=0.5\n",
    "        # - Final rating clipped to valid range [1,5] and rounded\n",
    "        w = popularity_weights[brands.index(brand)]\n",
    "        rating = np.clip(np.random.normal(3 + 2*w, 0.5), 1, 5)\n",
    "        data.append({'user_id': user_id, \n",
    "                    'brand': brand,\n",
    "                    'rating': round(rating)})  # Round to whole numbers\n",
    "\n",
    "# Convert to DataFrame and show summary\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Generated {len(df)} ratings from {num_users} users\")\n",
    "df.head()  # Display first few entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic DataFrame into Surprise's Dataset format\n",
    "# Reader tells Surprise the rating scale used in the DataFrame\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# Dataset.load_from_df expects columns in the order: user, item, rating\n",
    "data = Dataset.load_from_df(df[['user_id', 'brand', 'rating']], reader)\n",
    "\n",
    "# Split into training and test sets; test_size=0.2 uses 20% of ratings as hold-out\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train a matrix-factorization model (SVD) on the training set\n",
    "# SVD here is from surprise and implements a baseline matrix factorization algorithm\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# ---- Demo recommendations for a few sample users ----\n",
    "# We choose a representative set of user ids across the range to demonstrate predictions\n",
    "user_ids = [1, 42, 100, 250, 500]  # Example user ids to inspect recommendations\n",
    "all_brands = brands  # use the existing brands list generated earlier\n",
    "\n",
    "# For each user, compute predictions for items the user has NOT rated and show top-3\n",
    "for user_id in user_ids:\n",
    "    # Extract brands this user already rated so we avoid recommending them\n",
    "    already_rated = df[df['user_id'] == user_id]['brand'].tolist()\n",
    "\n",
    "    # Predict rating for each brand the user hasn't rated yet\n",
    "    preds = [model.predict(user_id, b) for b in all_brands if b not in already_rated]\n",
    "\n",
    "    # Sort predictions by estimated rating and pick top 3\n",
    "    top3 = sorted(preds, key=lambda x: x.est, reverse=True)[:3]\n",
    "\n",
    "    # Print nicely\n",
    "    print(f\"\\nTop-3 recommendations for User {user_id}:\")\n",
    "    for i, p in enumerate(top3, 1):\n",
    "        # p.iid is the item id (brand), p.est is the predicted rating\n",
    "        print(f\"  {i}. {p.iid}  →  {p.est:.2f}\")\n",
    "\n",
    "# Note: the block below was previously duplicated in the cell. The loop above\n",
    "# already prints recommendations for the selected users, so we won't repeat it.\n",
    "\n",
    "# ---- Evaluation on the hold-out test set ----\n",
    "# Use the trained model to predict ratings for the test set (the hold-out 20%)\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# RMSE is a common accuracy metric for rating prediction\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "\n",
    "# Compute nDCG per user to measure ranking quality\n",
    "# We'll skip users with only a single test item because nDCG requires at least 2 items\n",
    "ndcgs = []\n",
    "for uid in {p.uid for p in predictions}:\n",
    "    # Collect all predictions for this user\n",
    "    ups = [p for p in predictions if p.uid == uid]\n",
    "    if len(ups) > 1:\n",
    "        # true relevance scores and estimated scores\n",
    "        true = [p.r_ui for p in ups]\n",
    "        est  = [p.est  for p in ups]\n",
    "        # ndcg_score expects a list-of-lists for multiple queries, so wrap in []\n",
    "        ndcgs.append(ndcg_score([true], [est]))\n",
    "\n",
    "mean_ndcg = np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "print(\"\\nEvaluation on 20% hold-out:\")\n",
    "print(f\"  RMSE      : {rmse:.3f}\")\n",
    "print(f\"  Mean nDCG : {mean_ndcg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model Evaluation on Hold-out Test Set ----\n",
    "# Use the trained SVD model to predict ratings for all user-brand pairs in the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Calculate Root Mean Square Error (RMSE)\n",
    "# RMSE measures the average magnitude of prediction errors:\n",
    "# - Lower is better (0 would be perfect predictions)\n",
    "# - Typical values for 5-star rating systems: 0.8-1.2\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "\n",
    "# Calculate Normalized Discounted Cumulative Gain (nDCG)\n",
    "# nDCG measures ranking quality:\n",
    "# - Ranges from 0 to 1 (1 = perfect ranking)\n",
    "# - Takes into account both relevance and position in ranked list\n",
    "# - DCG penalizes relevant items appearing lower in the list\n",
    "ndcgs = []\n",
    "\n",
    "# Process predictions per user (we need multiple predictions per user for meaningful ranking)\n",
    "for uid in {p.uid for p in predictions}:\n",
    "    # Get all predictions for this user from test set\n",
    "    ups = [p for p in predictions if p.uid == uid]\n",
    "    \n",
    "    # Skip users with only one test item (can't evaluate ranking with single item)\n",
    "    if len(ups) > 1:\n",
    "        # Extract ground truth ratings and model predictions\n",
    "        true = [p.r_ui for p in ups]  # r_ui = real user rating\n",
    "        est  = [p.est  for p in ups]  # est = estimated rating\n",
    "        \n",
    "        # Calculate nDCG for this user\n",
    "        # scikit-learn expects [truth] and [prediction] as lists of lists\n",
    "        # because it's designed for multiple queries/rankings\n",
    "        ndcgs.append(ndcg_score([true], [est]))\n",
    "\n",
    "# Calculate mean nDCG across all eligible users\n",
    "# If no users had multiple test items, default to 0\n",
    "mean_ndcg = np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nEvaluation on 20% hold-out:\")\n",
    "print(f\"  RMSE      : {rmse:.3f}   # How accurate are the predicted ratings?\")\n",
    "print(f\"  Mean nDCG : {mean_ndcg:.3f}   # How good is the ranking quality?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
